# -*- coding: utf-8 -*-
"""
ç”¨äºåœ¨ç½‘ç»œæµé‡æ•°æ®ä¸Šå¾®è°ƒ GPT-2 æ¨¡å‹çš„è®­ç»ƒè„šæœ¬ï¼ˆä¼šè¯çº§åºåˆ—å»ºæ¨¡ï¼‰ã€‚

è¯´æ˜ï¼š
- è®­ç»ƒæ•°æ®æ¥è‡ªä¼šè¯çº§èšåˆåçš„æ–‡æœ¬ï¼šæ¯ä¸ªæ ·æœ¬å½¢å¦‚
    <bos>
    flow_line_1
    flow_line_2
    ...
    <eos>
  GPT-2 å­¦ä¹ çš„æ˜¯ã€Œä¼šè¯å†…æŒ‰æ—¶é—´æ’åºçš„æµåºåˆ—ã€çš„è”åˆåˆ†å¸ƒã€‚
- GPT-2 å†…éƒ¨è‡ªå¸¦ç»å¯¹ä½ç½®ç¼–ç ï¼Œé…åˆé¢„å¤„ç†é˜¶æ®µå›ºå®šçš„ç‰¹å¾åˆ—é¡ºåºï¼Œ
  å¯ä»¥è®©æ¨¡å‹å­¦ä¹ åˆ°â€œåŒä¸€è¡Œä¸­ä¸åŒä½ç½®å¯¹åº”ä¸åŒç‰¹å¾å­—æ®µâ€çš„éšå¼ç»“æ„ã€‚
- æœ¬è„šæœ¬åŒæ—¶æä¾›è°ƒè¯•ç”¨çš„å°æ ·æœ¬è®­ç»ƒå¼€å…³ï¼Œä¾¿äºå¿«é€Ÿè·‘é€šç«¯åˆ°ç«¯æµç¨‹ã€‚
"""
import torch
import os
import json
import csv
import numpy as np
import random
import matplotlib.pyplot as plt
import platform
from tqdm import tqdm
from transformers import (
    GPT2Tokenizer,
    GPT2LMHeadModel,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    DataCollatorForLanguageModeling
)
from torch.utils.data import Dataset
from datetime import datetime
import traceback
import time

# å¯¼å…¥è¯„ä¼°æ¨¡å—ä¸­çš„å›°æƒ‘åº¦è®¡ç®—å‡½æ•°
from evaluation import compute_perplexity

# --- 1. å…¨å±€è®¾ç½®ä¸éšæœºç§å­ ---
# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒç»“æœçš„å¯å¤ç°æ€§
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)

# è®¾ç½®matplotlibå­—ä½“ä»¥æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡å­—ç¬¦
# å¦‚æœåœ¨éWindows/macOSç³»ç»Ÿä¸Šï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å®‰è£…ä¸­æ–‡å­—ä½“


# --- 1.1 è°ƒè¯•ï¼é‡‡æ ·é…ç½® ---
# å°† DEBUG_SAMPLE_RATIO è®¾ä¸º < 1.0 å³å¯åœ¨è®­ç»ƒé˜¶æ®µåªç”¨éƒ¨åˆ†ä¼šè¯æ ·æœ¬ï¼Œä¾¿äºå¿«é€Ÿè°ƒè¯•ã€‚
# æ­£å¼å®éªŒæ—¶ï¼Œè¯·æ”¹å› 1.0ã€‚
DEBUG_SAMPLE_RATIO = 0.1  # ä¾‹å¦‚ 0.1 è¡¨ç¤ºåªç”¨ 10% çš„ä¼šè¯æ ·æœ¬

# --- 2. è‡ªå®šä¹‰æ•°æ®é›† ---
class FlowDataset(Dataset):
    """
    è‡ªå®šä¹‰æ•°æ®é›†ï¼Œç”¨äºåŠ è½½å’Œå¤„ç†ã€Œä¼šè¯çº§ã€ç½‘ç»œæµåºåˆ—ã€‚
    æ¯ä¸ªç”± <bos> å’Œ <eos> åŒ…å›´çš„å®Œæ•´ä¼šè¯è¢«è§†ä¸ºä¸€ä¸ªç‹¬ç«‹æ ·æœ¬ã€‚
    æ ·æœ¬å†…éƒ¨æ˜¯æŒ‰æ—¶é—´æ’åºçš„å¤šè¡Œæµç‰¹å¾æ–‡æœ¬ï¼ˆValues-only æ ¼å¼ï¼‰ã€‚
    """
    def __init__(self, tokenizer: GPT2Tokenizer, file_path: str, block_size: int = 1024):
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")

        print(f"æ­£åœ¨ä» {file_path} åŠ è½½æ•°æ®...")
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²ã€Œä¼šè¯ã€ï¼Œæ¯ä¸ªå—ä¸­åº”åŒ…å« <bos> ... <eos>
        sessions = text.split('\n\n')
        all_examples = [sess for sess in sessions if sess.strip()]

        # è°ƒè¯•æ¨¡å¼ï¼šä»…ä½¿ç”¨éƒ¨åˆ†ä¼šè¯æ ·æœ¬
        if 0 < DEBUG_SAMPLE_RATIO < 1.0:
            n_total = len(all_examples)
            n_keep = max(1, int(n_total * DEBUG_SAMPLE_RATIO))
            self.examples = all_examples[:n_keep]
            print(f"è°ƒè¯•æ¨¡å¼å¯ç”¨: ä»…ä½¿ç”¨ {n_keep}/{n_total} ä¸ªä¼šè¯æ ·æœ¬è¿›è¡Œ GPT-2 è®­ç»ƒã€‚")
        else:
            self.examples = all_examples
        
        self.tokenizer = tokenizer
        self.block_size = block_size
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…±æ‰¾åˆ° {len(self.examples)} ä¸ªä¼šè¯æ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        flow_text = self.examples[i]
        
        # å¯¹å•ä¸ªæµæ–‡æœ¬è¿›è¡Œåˆ†è¯
        # è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'input_ids' å’Œ 'attention_mask'
        tokenized_output = self.tokenizer(
            flow_text,
            max_length=self.block_size,
            truncation=True,
            padding=False, # DataCollatorä¼šå¤„ç†æ‰¹æ¬¡å†…çš„å¡«å……
        )
        return tokenized_output

# --- 3. è‡ªå®šä¹‰è®­ç»ƒå›è°ƒå‡½æ•° ---
class TrainingProgressCallback(TrainerCallback):
    """
    ä¸€ä¸ªç»Ÿä¸€çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºï¼š
    - æ˜¾ç¤ºè¯¦ç»†çš„è®­ç»ƒé…ç½®ã€‚
    - ç®¡ç†ä¸€ä¸ªæ€»çš„è®­ç»ƒè¿›åº¦æ¡ã€‚
    - åœ¨æ¯ä¸ªè¯„ä¼°ç‚¹å’Œepochç»“æŸæ—¶æ‰“å°æ ¼å¼åŒ–çš„æ—¥å¿—ã€‚
    - è®­ç»ƒç»“æŸåç”Ÿæˆå¹¶ä¿å­˜æŸå¤±å’Œå­¦ä¹ ç‡æ›²çº¿å›¾ã€‚
    """
    def __init__(self):
        self.pbar = None
        self.training_start_time = None
        self.epoch_start_time = None
        self.log_history = []

    def on_train_begin(self, args, state, control, **kwargs):
        self.training_start_time = time.time()
        self.pbar = tqdm(total=state.max_steps, desc="æ€»è®­ç»ƒè¿›åº¦", unit="æ­¥")

        print(f"\n{'='*80}")
        print(" " * 28 + "GPT-2 å¾®è°ƒä»»åŠ¡å¼€å§‹")
        print(f"{'='*80}")
        print(f"æ¨¡å‹è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"è®­ç»ƒè½®æ•° (Epochs): {args.num_train_epochs}")
        print(f"è®¾å¤‡æ‰¹æ¬¡å¤§å° (Batch Size): {args.per_device_train_batch_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {args.gradient_accumulation_steps}")
        print(f"æ€»ä¼˜åŒ–æ­¥æ•° (Total Steps): {state.max_steps}")
        print(f"å­¦ä¹ ç‡ (Learning Rate): {args.learning_rate}")
        print(f"è¯„ä¼°ä¸ä¿å­˜ç­–ç•¥: æ¯ {args.eval_steps} æ­¥")
        print(f"è®¾å¤‡: {'CUDA (' + torch.cuda.get_device_name(0) + ')' if torch.cuda.is_available() else 'CPU'}")
        print(f"ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒ (FP16): {args.fp16}")
        print(f"{'='*80}\n")
        
    def on_step_end(self, args, state, control, **kwargs):
        self.pbar.update(1)

    def on_epoch_begin(self, args, state, control, **kwargs):
        self.epoch_start_time = time.time()

    def on_evaluate(self, args, state, control, logs=None, **kwargs):
        # æ¯æ¬¡è¯„ä¼°åï¼Œæ‰“å°å¸¦å›°æƒ‘åº¦çš„æ—¥å¿—
        if logs and 'eval_loss' in logs:
            eval_loss = logs['eval_loss']
            perplexity = compute_perplexity(eval_loss)
            print(f"  [è¯„ä¼°] > æ­¥æ•°: {state.global_step}, "
                  f"éªŒè¯æŸå¤±: {eval_loss:.4f}, "
                  f"å›°æƒ‘åº¦ (Perplexity): {perplexity:.2f}")

    def on_log(self, args, state, control, logs=None, **kwargs):
        # æ”¶é›†æ‰€æœ‰æ—¥å¿—ï¼Œç”¨äºåç»­åˆ†æå’Œç»˜å›¾
        if logs:
            self.log_history.append({**logs, 'step': state.global_step})

    def on_epoch_end(self, args, state, control, **kwargs):
        epoch_duration = time.time() - self.epoch_start_time
        current_epoch_logs = [
            log for log in self.log_history 
            if 'loss' in log and int(log.get('epoch', 0)) == int(state.epoch)
        ]
        if current_epoch_logs:
            avg_train_loss = np.mean([log['loss'] for log in current_epoch_logs])
            print(f"\n--- Epoch {int(state.epoch)}/{int(state.num_train_epochs)} å®Œæˆ "
                  f"(è€—æ—¶: {epoch_duration:.2f}s) ---")
            print(f"  [æ€»ç»“] > å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print("-" * (40 + len(str(int(state.epoch))) + len(str(int(state.num_train_epochs)))))
            

    def on_train_end(self, args, state, control, **kwargs):
        self.pbar.close()
        total_training_time = time.time() - self.training_start_time
        
        print(f"\n{'='*80}")
        print(" " * 32 + "è®­ç»ƒå®Œæˆ")
        print(f"{'='*80}")
        print(f"æ€»è€—æ—¶: {total_training_time / 60:.2f} åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
        best_model_checkpoint = state.best_model_checkpoint
        print(f"æ€§èƒ½æœ€ä½³çš„æ¨¡å‹æ£€æŸ¥ç‚¹: {best_model_checkpoint}")
        
        # ä»logå†å²ä¸­æå–æœ€ç»ˆçš„è¯„ä¼°æŒ‡æ ‡
        final_eval_logs = next((log for log in reversed(self.log_history) if 'eval_loss' in log), None)
        if final_eval_logs:
            final_eval_loss = final_eval_logs['eval_loss']
            final_perplexity = compute_perplexity(final_eval_loss)
            print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_eval_loss:.4f}")
            print(f"æœ€ç»ˆå›°æƒ‘åº¦ (Perplexity): {final_perplexity:.2f}")
        print(f"{'='*80}\n")
        
        self.plot_curves(args.output_dir)
        
        # New: Save log_history to CSV
        csv_path = os.path.join(args.output_dir, 'training_log.csv')
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['step', 'epoch', 'loss', 'eval_loss', 'learning_rate', 'perplexity']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for log in self.log_history:
                row = {k: log.get(k, '') for k in fieldnames}
                if 'eval_loss' in log:
                    row['perplexity'] = compute_perplexity(log['eval_loss'])
                writer.writerow(row)
        print(f"âœ… è®­ç»ƒæ—¥å¿—å·²ä¿å­˜è‡³: {csv_path}")

    def plot_curves(self, output_dir):
        """ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿"""
        print("æ­£åœ¨ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...")
        os.makedirs(output_dir, exist_ok=True)

        train_steps = [log['step'] for log in self.log_history if 'loss' in log]
        train_losses = [log['loss'] for log in self.log_history if 'loss' in log]
        eval_steps = [log['step'] for log in self.log_history if 'eval_loss' in log]
        eval_losses = [log['eval_loss'] for log in self.log_history if 'eval_loss' in log]
        lr_steps = [log['step'] for log in self.log_history if 'learning_rate' in log]
        learning_rates = [log['learning_rate'] for log in self.log_history if 'learning_rate' in log]

        if not train_steps or not eval_steps:
            print("æ—¥å¿—æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæ›²çº¿å›¾ã€‚")
            return

        fig, ax1 = plt.subplots(figsize=(12, 8))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        ax1.plot(train_steps, train_losses, 'b-', alpha=0.6, label='Training Loss')
        ax1.plot(eval_steps, eval_losses, 'r-o', linewidth=2, label='Validation Loss')
        ax1.set_xlabel('Steps', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12, color='b')
        ax1.tick_params(axis='y', labelcolor='b')
        # åªåœ¨ä¸»è½´(ax1)ä¸Šç»˜åˆ¶Xè½´ç½‘æ ¼çº¿ï¼Œé¿å…åŒYè½´ç½‘æ ¼çº¿å†²çª
        ax1.grid(True, axis='x', linestyle='--', linewidth=0.5, alpha=0.7)
        ax1.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)

        # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
        ax2 = ax1.twinx()
        ax2.plot(lr_steps, learning_rates, 'g--', alpha=0.7, label='Learning Rate')
        ax2.set_ylabel('Learning Rate', fontsize=12, color='g')
        ax2.tick_params(axis='y', labelcolor='g')
        # ä¸åœ¨ç¬¬äºŒä¸ªYè½´ä¸Šç»˜åˆ¶ç½‘æ ¼çº¿ï¼Œé¿å…ä¸ax1çš„ç½‘æ ¼çº¿é‡å é€ æˆæ··ä¹±
        ax2.grid(False)
        
        fig.suptitle('GPT-2 Fine-Tuning Training Curves', fontsize=16, fontweight='bold')
        fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9), 
                   frameon=True, fancybox=True, framealpha=0.9, edgecolor='black')
        fig.tight_layout(rect=[0, 0, 1, 0.96])
        
        plot_path = os.path.join(output_dir, 'training_curves.png')
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"âœ… è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜è‡³: {plot_path}")


# --- 4. æ¨¡å‹åˆå§‹åŒ– ---
def initialize_model_and_tokenizer(model_path: str):
    """ä»æœ¬åœ°è·¯å¾„åŠ è½½é¢„è®­ç»ƒçš„GPT-2æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚"""
    print(f"æ­£åœ¨ä» '{model_path}' åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    
    # æ·»åŠ å¹¶è®¾ç½®ç‰¹æ®Štokens
    special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>'}
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
    
    model = GPT2LMHeadModel.from_pretrained(model_path)
    # è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚å¤§å°ä»¥åŒ¹é…æ–°çš„åˆ†è¯å™¨
    model.resize_token_embeddings(len(tokenizer))
    # ç¡®ä¿æ¨¡å‹é…ç½®çŸ¥é“æ–°çš„pad_token_id
    model.config.pad_token_id = tokenizer.pad_token_id

    print(f"âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆã€‚æ–°å¢äº† {num_added_toks} ä¸ªç‰¹æ®Š Tokenã€‚")
    return model, tokenizer

# --- 5. æ ¸å¿ƒè®­ç»ƒå‡½æ•° ---
def fine_tune_gpt2(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, 
                   train_file: str, val_file: str,
                   output_dir: str, num_epochs: int):
    """
    ä½¿ç”¨å‡†å¤‡å¥½çš„æ•°æ®é›†å¯¹GPT-2æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
    """
    # åŠ è½½æ•°æ®é›†
    train_dataset = FlowDataset(tokenizer=tokenizer, file_path=train_file)
    val_dataset = FlowDataset(tokenizer=tokenizer, file_path=val_file)

    # æ•°æ®æ•´ç†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……å’Œåˆ›å»ºlabels
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # åŠ¨æ€è®¡ç®—è¯„ä¼°æ­¥æ•°ï¼šæ¯è½®è¯„ä¼°2æ¬¡
    steps_per_epoch = len(train_dataset) // (8 * 1) # å‡è®¾BS=8, GradAccum=1
    eval_steps = max(10, steps_per_epoch // 2) 

    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=1,
        
        # åŒæ­¥æ—¥å¿—ã€è¯„ä¼°å’Œä¿å­˜ç­–ç•¥
        logging_strategy="steps",
        evaluation_strategy="steps",
        save_strategy="steps",
        
        # è®¾ç½®æ­¥æ•°
        logging_steps=eval_steps,
        eval_steps=eval_steps,
        save_steps=eval_steps,
        
        save_total_limit=2, # åªä¿ç•™æœ€æ–°çš„2ä¸ªæ£€æŸ¥ç‚¹
        load_best_model_at_end=True, # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_loss",
        
        learning_rate=5e-5,
        warmup_steps=500,
        lr_scheduler_type="cosine",
        
        fp16=torch.cuda.is_available(),
        
        # ä¼˜åŒ–é¡¹
        gradient_checkpointing=True, # èŠ‚çœæ˜¾å­˜
        dataloader_num_workers=2 if platform.system() != 'Windows' else 0, # Windowsä¸‹è®¾ä¸º0
        dataloader_pin_memory=True,

        report_to="none", # ç¦ç”¨wandbç­‰æŠ¥å‘Š
        disable_tqdm=True, # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„å›è°ƒè¿›åº¦æ¡
        log_level="error",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        callbacks=[TrainingProgressCallback()],
    )

    print("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # è®­ç»ƒç»“æŸåï¼Œä¿å­˜æœ€ç»ˆçš„æœ€ä½³æ¨¡å‹
    final_model_dir = os.path.join(output_dir, "best-model")
    trainer.save_model(final_model_dir)
    tokenizer.save_pretrained(final_model_dir)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_dir}")

    return trainer

# --- 6. ä¸»æ‰§è¡Œå‡½æ•° ---
def main():
    print(f"\n{'='*80}")
    print(" " * 25 + "GPT-2 ç½‘ç»œæµé‡å¾®è°ƒç¨‹åº (ä¼˜åŒ–ç‰ˆ)")
    print(f"{'='*80}\n")
    
    # --- æ–‡ä»¶è·¯å¾„é…ç½® ---
    # æ”¯æŒé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ•°æ®é›†åç§°
    import argparse
    parser = argparse.ArgumentParser(description='GPT-2 ç½‘ç»œæµé‡å¾®è°ƒç¨‹åº')
    parser.add_argument('--dataset', type=str, default='NF-UNSW-NB15-v3',
                       help='æ•°æ®é›†åç§°ï¼ˆä¾‹å¦‚: NF-UNSW-NB15-v3ï¼‰ï¼Œé»˜è®¤: NF-UNSW-NB15-v3')
    args = parser.parse_args()
    
    dataset_name = args.dataset
    train_file = f'./processed_data/{dataset_name}/train_gpt2_input.txt'
    val_file = f'./processed_data/{dataset_name}/val_gpt2_input.txt'
    local_model_path = './models/base/gpt2'
    output_dir = f"./models/gpt2-finetuned-traffic-{dataset_name}"

    # --- è·¯å¾„æ£€æŸ¥ ---
    paths_to_check = {
        "è®­ç»ƒæ–‡ä»¶": train_file,
        "éªŒè¯æ–‡ä»¶": val_file,
        "æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹": local_model_path,
    }
    for name, path in paths_to_check.items():
        if not os.path.exists(path):
            print(f"âŒ é”™è¯¯: {name}æœªæ‰¾åˆ°! è·¯å¾„: {path}")
            print("è¯·ç¡®ä¿å·²ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿è¡Œäº†æ•°æ®é¢„å¤„ç†è„šæœ¬ã€‚")
            return
            
    try:
        model, tokenizer = initialize_model_and_tokenizer(local_model_path)
        
        fine_tune_gpt2(
            model=model,
            tokenizer=tokenizer,
            train_file=train_file,
            val_file=val_file,
            output_dir=output_dir,
            num_epochs=5,
        )
        
        print("\nğŸ‰ å¾®è°ƒä»»åŠ¡æˆåŠŸå®Œæˆ!")
        print(f"\nä¸‹ä¸€æ­¥:\n1. æŸ¥çœ‹ '{output_dir}' ç›®å½•ä¸‹çš„è®­ç»ƒæ›²çº¿å›¾å’Œæ—¥å¿—ã€‚")
        print(f"2. ä½¿ç”¨ '{os.path.join(output_dir, 'best-model')}' è·¯å¾„ä¸‹çš„æ¨¡å‹è¿›è¡Œæµé‡ç”Ÿæˆã€‚")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
