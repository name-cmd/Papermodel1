import os
# é™é»˜ tokenizer å¹¶è¡Œåˆ†å‰å‘Šè­¦
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# è¿›åº¦æ¡æ˜¾ç¤ºä¼˜åŒ–ï¼šä½¿ç”¨ ASCII å­—ç¬¦æ¨¡å¼ï¼Œå›ºå®šå®½åº¦ï¼Œæé«˜ç»ˆç«¯å…¼å®¹æ€§
# è§£å†³å±å¹•åˆ‡æ¢ã€é»‘å±ç­‰åœºæ™¯ä¸‹è¿›åº¦æ¡ä¹±ç ã€ä¸æ˜¾ç¤ºã€å¾ªç¯å¤§é‡æ˜¾ç¤ºç­‰é—®é¢˜
os.environ["TQDM_ASCII"] = "true"
os.environ["TQDM_NCOLS"] = "100"
import argparse
import json
import time
from pathlib import Path
import shutil
import csv # Added for saving training log

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    EarlyStoppingCallback,
    set_seed
)

import matplotlib.pyplot as plt
import seaborn as sns

# é…ç½®ï¼ˆä¸ notebooks æ€è·¯å¯¹é½ï¼Œä½†ä¿ç•™ BERT/DistilBERTï¼‰
MODEL_PATH = "./models/base/distilbert"  # é¢„è®­ç»ƒ DistilBERT ç›®å½•ï¼ˆå·²åœ¨é¡¹ç›®ä¸­ï¼‰
BERT_BASE_DIR = "./processed_data/bert"  # (å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹)
PROCESSED_DATA_BASE_DIR = "./processed_data"  # é¢„å¤„ç†æ•°æ®åŸºç¡€ç›®å½•
# æ ¹æ® mode è‡ªåŠ¨é€‰æ‹©å­ç›®å½•ï¼šbinary -> bert/binary, multiclass -> bert/multiclass
OUTPUT_DIR_BINARY = "./models/distilbert-finetuned"  # äºŒåˆ†ç±»å¾®è°ƒè¾“å‡º
OUTPUT_DIR_MULTICLASS = "./models/distilbert-finetuned-multiclass"  # å¤šåˆ†ç±»å¾®è°ƒè¾“å‡º
# è®­ç»ƒæ¨¡å¼ï¼šbinary æˆ– multiclassï¼ˆä½œä¸ºä¿¡æ¯æç¤ºä¸ä¸€è‡´æ€§æ ¡éªŒï¼‰
DEFAULT_MODE = "binary"
DEFAULT_DATASET = "NF-CICIDS2018-v3"  # é»˜è®¤æ•°æ®é›†åç§°/NF-CICIDS2018-v3
NUM_EPOCHS = 6
RANDOM_SEED = 42

# --- è°ƒè¯•/é‡‡æ ·é…ç½® ---
# å°† DEBUG_SAMPLE_RATIO è®¾ä¸º < 1.0 å³å¯åªç”¨éƒ¨åˆ†æ ·æœ¬ï¼Œä¾¿äºå¿«é€Ÿè°ƒè¯•è·‘é€šæµç¨‹ã€‚
# æ­£å¼å®éªŒæ—¶ï¼Œè¯·æ”¹å› 1.0ã€‚
DEBUG_SAMPLE_RATIO = 0.4  # ä¾‹å¦‚ 0.1 è¡¨ç¤ºåªç”¨ 10% çš„æ ·æœ¬


def get_bert_dir(dataset_name: str, mode: str) -> str:
    """æ ¹æ®æ•°æ®é›†åç§°å’Œè®­ç»ƒæ¨¡å¼è¿”å›å¯¹åº”çš„æ•°æ®ç›®å½•"""
    return os.path.join(PROCESSED_DATA_BASE_DIR, dataset_name, 'bert', mode)


def get_output_dir(mode: str) -> str:
    """æ ¹æ®è®­ç»ƒæ¨¡å¼è¿”å›å¯¹åº”çš„è¾“å‡ºç›®å½•"""
    if mode == 'binary':
        return OUTPUT_DIR_BINARY
    else:
        return OUTPUT_DIR_MULTICLASS

class PacketDataset(Dataset):
    """å°†é¢„å¤„ç†åçš„ 'text' æ‹¼æ¥ç‰¹å¾é€å…¥ BERT çš„æ•°æ®é›†å°è£…"""
    def __init__(self, tokenizer, texts, labels, max_length=512):
        self.tokenizer = tokenizer
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        enc = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': enc['input_ids'].squeeze(0),
            'attention_mask': enc['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class DetailedLoggingCallback(TrainerCallback):
    """è®°å½•æ¯ä¸ª epoch çš„è®­ç»ƒä¸éªŒè¯åº¦é‡ï¼Œä¿è¯æŒ‡æ ‡æŒ‰è½®æ¬¡å˜åŒ–ã€‚
    
    ä½¿ç”¨ flush=True ç¡®ä¿æ—¥å¿—åŠæ—¶è¾“å‡ºï¼Œé¿å…ç¼“å†²åŒºé—®é¢˜ã€‚
    """
    def __init__(self):
        self.epoch_start_time = None
        self.train_start_time = None
        self.current_epoch = 0
        self.total_epochs = 0
        self.epochs = []
        self.train_losses = []
        self.val_losses = []
        self.val_accs = []
        self.val_f1s = []
    
    def on_train_begin(self, args, state, control, **kwargs):
        self.train_start_time = time.time()
        self.total_epochs = int(args.num_train_epochs)
        print("\n" + "="*80, flush=True)
        print("å¼€å§‹è®­ç»ƒ", flush=True)
        print("="*80, flush=True)
        print(f"é…ç½®: epochs={args.num_train_epochs} | batch={args.per_device_train_batch_size} | lr={args.learning_rate} | scheduler={args.lr_scheduler_type}", flush=True)
        print(f"æ€»æ­¥æ•°: {state.max_steps}", flush=True)
        print("-" * 80, flush=True)
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        # é¿å… int(state.epoch)==0 å¯¼è‡´è¿ç»­æ‰“å° 0ï¼›ä½¿ç”¨æœ¬åœ°è®¡æ•°å™¨ç»Ÿä¸€é€’å¢
        self.current_epoch += 1
        self.epoch_start_time = time.time()
        print(f"\nğŸ“ Epoch {self.current_epoch}/{self.total_epochs} å¼€å§‹...", flush=True)
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        # é™ä½æ—¥å¿—é¢‘ç‡ï¼šä¸åœ¨æ¯ä¸ª step æ‰“å°
        return
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and self.epoch_start_time:
            epoch_duration = time.time() - self.epoch_start_time
            total_elapsed = time.time() - self.train_start_time if self.train_start_time else 0
            val_loss = float(metrics.get('eval_loss', 0))
            val_acc = float(metrics.get('eval_accuracy', 0))
            val_f1 = float(metrics.get('eval_f1', 0))
            val_prec = float(metrics.get('eval_precision', 0))
            val_rec = float(metrics.get('eval_recall', 0))
            # ä» state.log_history æ‰¾æœ€è¿‘çš„è®­ç»ƒ loss
            train_loss = next((float(log['loss']) for log in reversed(state.log_history) if 'loss' in log), None)
            if train_loss is None:
                train_loss = float('nan')
            self.epochs.append(self.current_epoch)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)
            self.val_f1s.append(val_f1)
            # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
            if self.current_epoch > 0:
                avg_epoch_time = total_elapsed / self.current_epoch
                eta = avg_epoch_time * (self.total_epochs - self.current_epoch)
            else:
                eta = 0
            print(f"  Train Loss: {train_loss:.4f}", flush=True)
            print(f"  Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}", flush=True)
            print(f"  Precision: {val_prec:.4f} | Recall: {val_rec:.4f}", flush=True)
            print(f"  â±ï¸  æœ¬è½®è€—æ—¶: {epoch_duration:.1f}s | æ€»å·²ç”¨æ—¶: {total_elapsed/60:.1f}min | é¢„è®¡å‰©ä½™: {eta/60:.1f}min", flush=True)
    
    def on_train_end(self, args, state, control, **kwargs):
        total_time = time.time() - self.train_start_time if self.train_start_time else 0
        best_val_loss = min(self.val_losses) if self.val_losses else float('nan')
        best_val_f1 = max(self.val_f1s) if self.val_f1s else float('nan')
        print("\n" + "="*80, flush=True)
        print(f"âœ“ è®­ç»ƒå®Œæˆ | æ€»è€—æ—¶: {total_time/60:.1f}min | æœ€ä½³: Val Loss={best_val_loss:.4f}, F1={best_val_f1:.4f}", flush=True)
        print("="*80, flush=True)
        
        # Save to CSV
        csv_path = os.path.join(args.output_dir, 'training_log.csv')
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['epoch', 'train_loss', 'val_loss', 'val_acc', 'val_f1']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for i in range(len(self.epochs)):
                writer.writerow({
                    'epoch': self.epochs[i],
                    'train_loss': self.train_losses[i],
                    'val_loss': self.val_losses[i],
                    'val_acc': self.val_accs[i],
                    'val_f1': self.val_f1s[i]
                })
        print(f"âœ… è®­ç»ƒæ—¥å¿—å·²ä¿å­˜è‡³: {csv_path}", flush=True)
    
    def get_plot_data(self):
        return {
            'epochs': self.epochs,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accs': self.val_accs,
            'val_f1s': self.val_f1s
        }
class FocalLoss(nn.Module):
    """å¤šåˆ†ç±» Focal Lossã€‚
    å‚æ•°:
      gamma: èšç„¦å‚æ•°ï¼Œè¶Šå¤§è¶Šå…³æ³¨éš¾æ ·æœ¬ï¼ˆé»˜è®¤ 2.0ï¼‰
      alpha: å¯é€‰çš„æ¯ç±»æƒé‡å¼ é‡ï¼Œshape=[num_labels]ï¼›ä¸º None æ—¶ä¸ä½¿ç”¨ç±»æƒé‡
    """
    def __init__(self, gamma: float = 2.0, alpha: torch.Tensor = None, reduction: str = 'mean'):
        super().__init__()
        self.gamma = float(gamma)
        self.reduction = reduction
        # ä½¿ç”¨ register_buffer ç¡®ä¿ alpha éšæ¨¡å‹è‡ªåŠ¨è¿ç§»åˆ°æ­£ç¡®è®¾å¤‡ï¼ˆCPU/GPUï¼‰
        if alpha is not None:
            self.register_buffer('alpha', alpha)
        else:
            self.alpha = None

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)
        pt = torch.exp(-ce)  # é¢„æµ‹ä¸ºçœŸç±»çš„æ¦‚ç‡
        focal = ((1 - pt) ** self.gamma) * ce
        if self.alpha is not None:
            # alpha é€šè¿‡ register_buffer æ³¨å†Œï¼Œéšæ¨¡å‹è‡ªåŠ¨è¿ç§»è®¾å¤‡
            # æ­¤å¤„ä»…åšç´¢å¼•å–æƒé‡ï¼Œä¸ä¿®æ”¹ self.alpha
            alpha_weight = self.alpha[targets]
            focal = alpha_weight * focal
        if self.reduction == 'mean':
            return focal.mean()
        if self.reduction == 'sum':
            return focal.sum()
        return focal


def _ensure_int_labels(series: pd.Series) -> np.ndarray:
    """å°† label åˆ—è½¬ä¸º int numpy æ•°ç»„ï¼ˆè‹¥å­˜åœ¨è„æ•°æ®åˆ™å°½é‡å®¹é”™ï¼‰ã€‚"""
    # æ³¨æ„ï¼šè®­ç»ƒ/è¯„ä¼°éƒ½ä¾èµ– label ä¸ºä» 0 å¼€å§‹çš„éè´Ÿæ•´æ•°
    return pd.to_numeric(series, errors='coerce').fillna(0).astype(int).values


def _infer_num_labels_from_labels(all_labels: np.ndarray) -> int:
    """æ›´ç¨³å¥åœ°æ¨æ–­ num_labelsï¼š
    - è‹¥æ ‡ç­¾æ˜¯éè´Ÿæ•´æ•°ï¼šç”¨ max+1ï¼ˆå³ä½¿ä¸­é—´æœ‰ç¼ºå¤± id ä¹Ÿä¸ä¼šç»´åº¦ä¸åŒ¹é…ï¼‰
    - å¦åˆ™ï¼šé€€åŒ–ä¸º unique æ•°é‡
    """
    if all_labels.size == 0:
        return 0
    try:
        min_v = int(np.min(all_labels))
        max_v = int(np.max(all_labels))
        if min_v >= 0:
            return int(max_v + 1)
    except Exception:
        pass
    return int(np.unique(all_labels).size)


def load_bert_datasets(bert_dir: str, mode: str = 'binary'):
    """åŠ è½½ processed_data/bert ä¸‹çš„ train/val/test_fusion.csvï¼Œè¿”å› DataFrame åŠåŸºæœ¬ç»Ÿè®¡ã€‚

    å‚æ•°:
      bert_dir: processed_data/bert ç›®å½•
      mode: 'binary' æˆ– 'multiclass'

    çº¦å®š:
      - binary: ç»Ÿä¸€æ˜ å°„ä¸º 0=æ­£å¸¸, 1=æ”»å‡»ï¼ˆä»»ä½•é0éƒ½è§†ä¸ºæ”»å‡»ï¼‰
      - multiclass: ä¿ç•™åŸå§‹å¤šç±» labelï¼ˆåº”ä¸ºä» 0 å¼€å§‹çš„æ•´æ•° idï¼‰
    """
    def read_csv(path):
        try:
            df = pd.read_csv(path)
            print(f"  âœ“ {path}: {len(df)} è¡Œ")
            return df
        except Exception as e:
            print(f"  âœ— {path}: {e}")
            return None
    print(f"\nåŠ è½½æ•°æ®é›†...")
    train_df = read_csv(f"{bert_dir}/train_fusion.csv")
    val_df = read_csv(f"{bert_dir}/val_fusion.csv")
    test_df = read_csv(f"{bert_dir}/test_fusion.csv")
    if train_df is None or val_df is None or test_df is None:
        raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
    
    # è°ƒè¯•æ¨¡å¼ï¼šä»…ä½¿ç”¨éƒ¨åˆ†æ ·æœ¬ï¼ˆåˆ†å±‚é‡‡æ ·ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
    if 0 < DEBUG_SAMPLE_RATIO < 1.0:
        n_train_orig = len(train_df)
        n_val_orig = len(val_df)
        n_test_orig = len(test_df)
        
        def stratified_sample(df, frac, label_col='label'):
            """åˆ†å±‚é‡‡æ ·ï¼šä»æ¯ä¸ªç±»åˆ«ä¸­æŒ‰ç›¸åŒæ¯”ä¾‹æŠ½å–æ ·æœ¬"""
            sampled_dfs = []
            for label_val in df[label_col].unique():
                label_subset = df[df[label_col] == label_val]
                n_sample = max(1, int(len(label_subset) * frac))  # è‡³å°‘ä¿ç•™1ä¸ªæ ·æœ¬
                sampled_dfs.append(
                    label_subset.sample(n=n_sample, random_state=RANDOM_SEED)
                )
            return pd.concat(sampled_dfs, axis=0).sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)
        
        train_df = stratified_sample(train_df, DEBUG_SAMPLE_RATIO)
        val_df = stratified_sample(val_df, DEBUG_SAMPLE_RATIO)
        test_df = stratified_sample(test_df, DEBUG_SAMPLE_RATIO)
        
        print(f"\nâš ï¸  è°ƒè¯•æ¨¡å¼å¯ç”¨ (DEBUG_SAMPLE_RATIO={DEBUG_SAMPLE_RATIO}, åˆ†å±‚é‡‡æ ·):")
        print(f"  è®­ç»ƒé›†: {n_train_orig} -> {len(train_df)}")
        print(f"  éªŒè¯é›†: {n_val_orig} -> {len(val_df)}")
        print(f"  æµ‹è¯•é›†: {n_test_orig} -> {len(test_df)}")
    for df, name in [(train_df, 'train'), (val_df, 'val'), (test_df, 'test')]:
        if not all(col in df.columns for col in ['text', 'label']):
            raise ValueError(f"{name}.csv ç¼ºå°‘å¿…éœ€çš„åˆ—: ['text','label']")
    # ç»Ÿä¸€æ¸…æ´— label ä¸º int
    train_labels_raw = _ensure_int_labels(train_df['label'])
    val_labels_raw = _ensure_int_labels(val_df['label'])
    test_labels_raw = _ensure_int_labels(test_df['label'])

    if mode == 'binary':
        # äºŒåˆ†ç±»ï¼šä»»ä½•é0éƒ½è§†ä¸ºæ”»å‡»ç±» 1
        train_labels = np.where(train_labels_raw == 0, 0, 1)
        val_labels = np.where(val_labels_raw == 0, 0, 1)
        test_labels = np.where(test_labels_raw == 0, 0, 1)

        # è¦†å†™ DataFrameï¼Œä¿è¯åç»­ dataset / report å£å¾„ä¸€è‡´
        train_df = train_df.copy()
        val_df = val_df.copy()
        test_df = test_df.copy()
        train_df['label'] = train_labels
        val_df['label'] = val_labels
        test_df['label'] = test_labels

        label_counts = dict(zip(*np.unique(train_labels, return_counts=True)))
        print(f"\nè®­ç»ƒé›†ç»Ÿè®¡ (äºŒåˆ†ç±»æ¨¡å¼):")
        print(f"  æ€»æ ·æœ¬æ•°: {len(train_df)}")
        print(f"  ç±»åˆ«æ•°: 2 (0=æ­£å¸¸, 1=æ”»å‡»[é0])")
        print(f"  ç±»åˆ«åˆ†å¸ƒ: {label_counts}")

        # è‹¥åŸå§‹æ ‡ç­¾ä¸æ˜¯ {0,1}ï¼Œæ˜ç¡®æç¤ºç”¨æˆ·å½“å‰ CSV å®é™…ä¸Šæ¥è‡ªå¤šåˆ†ç±»é¢„å¤„ç†
        raw_unique = np.unique(train_labels_raw)
        if raw_unique.size > 2 or (raw_unique.size == 2 and not set(raw_unique.tolist()).issubset({0, 1})):
            print(f"  æç¤º: ä½ çš„ CSV åŸå§‹ label å»é‡åä¸º {raw_unique.tolist()}ï¼Œå·²åœ¨äºŒåˆ†ç±»æ¨¡å¼ä¸‹æ˜ å°„ä¸º 0/1ã€‚")
    else:
        # å¤šåˆ†ç±»ï¼šä¿ç•™åŸå§‹æ ‡ç­¾
        train_labels = train_labels_raw
        val_labels = val_labels_raw
        test_labels = test_labels_raw

        label_counts = dict(zip(*np.unique(train_labels, return_counts=True)))
        unique_labels = np.unique(train_labels)
        print(f"\nè®­ç»ƒé›†ç»Ÿè®¡ (å¤šåˆ†ç±»æ¨¡å¼):")
        print(f"  æ€»æ ·æœ¬æ•°: {len(train_df)}")
        print(f"  ç±»åˆ«æ•°: {len(unique_labels)}")
        print(f"  ç±»åˆ«åˆ†å¸ƒ: {label_counts}")
    if len(label_counts) > 0:
        max_count = max(label_counts.values())
        min_count = min(label_counts.values())
        if min_count > 0:
            print(f"  ä¸å¹³è¡¡åº¦: {max_count}/{min_count} = {max_count/min_count:.1f}x")
    # num_labelsï¼šbinary å›ºå®šä¸º 2ï¼›multiclass ç”¨ max+1 é˜²æ­¢æ ‡ç­¾ä¸è¿ç»­
    all_labels = np.concatenate([train_labels, val_labels, test_labels], axis=0)
    inferred_num_labels = 2 if mode == 'binary' else _infer_num_labels_from_labels(all_labels)

    return {
        # åŸå§‹ DataFrameï¼Œä¾¿äºåç»­è®¿é—®ä¼šè¯çº§å‰ç¼€ç‰¹å¾ç­‰æ•°å€¼åˆ—
        'train_df': train_df,
        'val_df': val_df,
        'test_df': test_df,
        # å…¼å®¹åŸæœ‰è°ƒç”¨æ–¹å¼çš„ tuple
        'train': (train_df['text'].values, train_labels),
        'val': (val_df['text'].values, val_labels),
        'test': (test_df['text'].values, test_labels),
        'num_labels': int(inferred_num_labels),
        'label_counts': label_counts
    }


def compute_class_weights(labels, num_labels, power):
    """ä½¿ç”¨ sklearn çš„ balanced ç­–ç•¥å¹¶è¿›è¡Œå¹‚å¹³æ»‘ï¼Œè¿”å›é•¿åº¦=num_labels çš„æƒé‡å¼ é‡"""
    unique_labels = np.unique(labels)
    base_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels)
    smoothed = np.power(base_weights, power)
    smoothed = smoothed / smoothed.mean() if smoothed.mean() > 0 else smoothed
    weights = np.ones(num_labels, dtype=np.float32)
    for label, w in zip(unique_labels, smoothed):
        if 0 <= int(label) < num_labels:
            weights[int(label)] = float(w)
    print(f"\nç±»åˆ«æƒé‡ (å¹³æ»‘ç³»æ•°={power}): èŒƒå›´[{weights.min():.4f},{weights.max():.4f}] æ¯”ä¾‹{(weights.max()/max(weights.min(),1e-6)):.2f}x")
    return torch.tensor(weights, dtype=torch.float32)


def _format_session_stat_value(v):
    """å°†ä¼šè¯çº§æ•°å€¼ç‰¹å¾æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œä¾›ä¼ª token ä½¿ç”¨ã€‚"""
    try:
        fv = float(v)
        if abs(fv - round(fv)) < 1e-6:
            return str(int(round(fv)))
        return f"{fv:.4f}"
    except Exception:
        return str(v)


def build_text_with_session_tokens(df: pd.DataFrame, session_cols, base_text_col: str = 'text'):
    """
    å°†æ•°å€¼å‹ä¼šè¯å‰ç¼€ç‰¹å¾ç¼–ç ä¸ºä¼ª tokenï¼Œå¹¶ä¸åŸå§‹æµçº§æ–‡æœ¬æ‹¼æ¥ã€‚
    ç¤ºä¾‹ï¼š
      åŸ text: "1 2 3 ..."
      ä¼šè¯ç‰¹å¾: SESS_FLOW_COUNT=10, SESS_TOTAL_BYTES=1234
      æ‹¼æ¥å: "1 2 3 ... [SEP] SESS_FLOW_COUNT=10 SESS_TOTAL_BYTES=1234"
    """
    if not session_cols:
        return df[base_text_col].astype(str).values

    df_local = df.copy()
    df_local[session_cols] = df_local[session_cols].fillna(0)

    def _row_to_session_str(row):
        parts = []
        for col in session_cols:
            if col in row:
                parts.append(f"{col}={_format_session_stat_value(row[col])}")
        return " ".join(parts)

    session_strings = df_local.apply(_row_to_session_str, axis=1)
    combined = df_local[base_text_col].astype(str) + " [SEP] " + session_strings
    return combined.values


def train_distilbert(model_path: str, bert_dir: str, output_dir: str, num_epochs: int,
                     mode: str = DEFAULT_MODE, dataset_name: str = DEFAULT_DATASET,
                     use_session_tokens: bool = True, loss_type: str = 'focal'):
    """ä½¿ç”¨ DistilBERT è¿›è¡Œï¼ˆå¤šï¼‰åˆ†ç±»å¾®è°ƒï¼Œæ”¯æŒå¯é€‰çš„ä¼šè¯å‰ç¼€ç‰¹å¾ä¼ª token èåˆã€‚"""
    set_seed(RANDOM_SEED)
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    # æ ¹æ® mode è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„æ•°æ®ç›®å½•ï¼ˆè‹¥ç”¨æˆ·æœªé€šè¿‡ bert_dir æ‰‹åŠ¨æŒ‡å®šï¼‰
    actual_bert_dir = bert_dir if bert_dir else get_bert_dir(dataset_name, mode)
    if not os.path.exists(actual_bert_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {actual_bert_dir}")
        print(f"è¯·å…ˆè¿è¡Œ data_preprocessing.py ç”Ÿæˆ {mode} æ¨¡å¼çš„æ•°æ®")
        return
    print(f"\nè®­ç»ƒæ¨¡å¼: {mode}")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"æ•°æ®ç›®å½•: {actual_bert_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    # åŠ è½½æ•°æ®
    data = load_bert_datasets(actual_bert_dir, mode=mode)
    train_df, val_df, test_df = data['train_df'], data['val_df'], data['test_df']
    num_labels = data['num_labels']

    # åŸºç¡€æ–‡æœ¬ä¸æ ‡ç­¾
    # æ³¨æ„ï¼šlabel å·²åœ¨ load_bert_datasets å†…æŒ‰ mode åšè¿‡æ¸…æ´—/æ˜ å°„ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨å³å¯
    train_labels = _ensure_int_labels(train_df['label'])
    val_labels = _ensure_int_labels(val_df['label'])
    test_labels = _ensure_int_labels(test_df['label'])

    # äºŒåˆ†ç±»ä¸‹å†åšä¸€æ¬¡æ–­è¨€å¼å…œåº•ï¼Œé˜²æ­¢å¤–éƒ¨è¯¯æ”¹å¯¼è‡´æ ‡ç­¾è¶Šç•Œ
    if mode == 'binary':
        train_labels = np.where(train_labels == 0, 0, 1)
        val_labels = np.where(val_labels == 0, 0, 1)
        test_labels = np.where(test_labels == 0, 0, 1)
        train_df = train_df.copy(); train_df['label'] = train_labels
        val_df = val_df.copy(); val_df['label'] = val_labels
        test_df = test_df.copy(); test_df['label'] = test_labels

    # ä»ç‰¹å¾é…ç½®ä¸­è¯»å–ä¼šè¯å‰ç¼€ç‰¹å¾åï¼ˆè‹¥å­˜åœ¨ï¼‰
    # actual_bert_dir æ˜¯ processed_data/bert/binary æˆ– multiclass
    # feature_columns.json åœ¨ processed_data/dataset_name/ ç›®å½•ä¸‹
    feature_cfg_path = Path(PROCESSED_DATA_BASE_DIR) / dataset_name / 'feature_columns.json'
    session_stat_cols = []
    if feature_cfg_path.exists():
        try:
            with open(feature_cfg_path, 'r', encoding='utf-8') as f:
                cfg = json.load(f)
            session_stat_cols = cfg.get('session_stat_features', [])
            print(f"\nä» {feature_cfg_path} è¯»å–ä¼šè¯å‰ç¼€ç‰¹å¾åˆ—: {session_stat_cols}")
        except Exception as e:
            print(f"è­¦å‘Š: è¯»å–ç‰¹å¾é…ç½®å¤±è´¥ ({e})ï¼Œå°†ä¸ä½¿ç”¨ä¼šè¯ç‰¹å¾ä¼ª tokenã€‚")
            use_session_tokens = False
            session_stat_cols = []
    else:
        if use_session_tokens:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° {feature_cfg_path}ï¼Œå°†ä¸ä½¿ç”¨ä¼šè¯ç‰¹å¾ä¼ª tokenã€‚")
        use_session_tokens = False

    # æ ¹æ®å¼€å…³ï¼Œå†³å®šæ˜¯å¦åœ¨æ–‡æœ¬ä¸­æ‹¼æ¥ä¼šè¯å‰ç¼€ä¼ª token
    if use_session_tokens and session_stat_cols:
        print("\nå¯ç”¨ä¼šè¯å‰ç¼€ç‰¹å¾ä¼ª token èåˆåˆ° BERT è¾“å…¥æ–‡æœ¬ä¸­...")
        train_texts = build_text_with_session_tokens(train_df, session_stat_cols)
        val_texts = build_text_with_session_tokens(val_df, session_stat_cols)
        test_texts = build_text_with_session_tokens(test_df, session_stat_cols)
    else:
        print("\nä¸ä½¿ç”¨ä¼šè¯ç‰¹å¾ä¼ª tokenï¼Œä»…åŸºäºå•æµçº§æ–‡æœ¬è®­ç»ƒï¼ˆå¯è§†ä¸ºæ¶ˆèè®¾ç½®ï¼‰...")
        train_texts = train_df['text'].astype(str).values
        val_texts = val_df['text'].astype(str).values
        test_texts = test_df['text'].astype(str).values
    # tokenizer/model: ä½¿ç”¨ Auto* ä»¥å…¼å®¹ BERT/DistilBERT æƒé‡ï¼›è‹¥æ¨¡å‹æ—  pad_tokenï¼Œè¿›è¡Œä¿®å¤
    print("\nåŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    print("åŠ è½½æ¨¡å‹...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path,
        num_labels=int(num_labels),
        problem_type="single_label_classification",
        ignore_mismatched_sizes=True
    )
    if tokenizer.pad_token is None and tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = model.config.eos_token_id
    model.to(device)
    # æ•°æ®é›†
    print("åˆ›å»ºæ•°æ®é›†...")
    train_dataset = PacketDataset(tokenizer, train_texts, train_labels)
    val_dataset = PacketDataset(tokenizer, val_texts, val_labels)
    test_dataset = PacketDataset(tokenizer, test_texts, test_labels)

    # -----------------------------------------------------------------------------
    # æ„é€  WeightedRandomSamplerï¼Œä»…ä½œç”¨äºè®­ç»ƒé›†ï¼š
    #  - ä¾æ®ç±»åˆ«é¢‘ç‡è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é‡‡æ ·æƒé‡ï¼›
    #  - é¢‘ç‡è¶Šå°ï¼Œæƒé‡è¶Šå¤§ï¼Œä»è€Œåœ¨è®­ç»ƒé˜¶æ®µè¿‡é‡‡æ ·å°‘æ•°ç±»ã€å¼±åŒ–å¤šæ•°ç±»ã€‚
    # -----------------------------------------------------------------------------
    print("\næ„å»ºè®­ç»ƒé›† WeightedRandomSampler ä»¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡...")
    train_label_tensor = torch.tensor(train_labels, dtype=torch.long)
    # ä½¿ç”¨ num_labels å…œåº•ï¼Œç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½è¢«ç»Ÿè®¡åˆ°
    if num_labels is not None and int(num_labels) > 0:
        class_counts = torch.bincount(train_label_tensor, minlength=int(num_labels)).float()
    else:
        class_counts = torch.bincount(train_label_tensor).float()
    # é¿å…é™¤ä»¥ 0
    class_counts = torch.clamp(class_counts, min=1.0)
    class_weights_for_sampler = 1.0 / class_counts
    sample_weights = class_weights_for_sampler[train_label_tensor]
    train_sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True  # å…è®¸è¿‡é‡‡æ ·å°‘æ•°ç±»
    )
    # æ ¹æ® loss_type é€‰æ‹©æŸå¤±å‡½æ•°
    focal_gamma = 2.0  # Focal Loss çš„ gamma å‚æ•°ï¼Œæ§åˆ¶èšç„¦ç¨‹åº¦

    if loss_type == 'ce':
        # çº¯äº¤å‰ç†µæŸå¤±
        loss_fn = nn.CrossEntropyLoss().to(device)
        print("ä½¿ç”¨çº¯äº¤å‰ç†µæŸå¤±å‡½æ•°")
    elif loss_type == 'focal':
        # æ— æƒé‡ç‰ˆæœ¬ï¼šFocal Loss (gamma=2, alpha=None)
        loss_fn = FocalLoss(gamma=focal_gamma, alpha=None).to(device)
        print(f"ä½¿ç”¨æ— æƒé‡ Focal Loss (gamma={focal_gamma})")
    elif loss_type == 'focal_weighted':
        # æœ‰æƒé‡ç‰ˆæœ¬ï¼šFocal Loss (gamma=2, alpha=class_weights)
        class_weights = compute_class_weights(train_labels, num_labels, power=0.25)
        # å°† FocalLoss ç§»åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ï¼Œç¡®ä¿ alpha buffer ä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        loss_fn = FocalLoss(gamma=focal_gamma, alpha=class_weights).to(device)
        print(f"ä½¿ç”¨æœ‰æƒé‡ Focal Loss (gamma={focal_gamma})")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_type}")

    class CustomLossTrainer(Trainer):
        def __init__(self, *args, loss_fn=None, train_sampler=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.loss_fn = loss_fn
            # ä»…ç”¨äºè®­ç»ƒé›†çš„æ•°æ®åŠ è½½ï¼›éªŒè¯/æµ‹è¯•ä»ä½¿ç”¨é»˜è®¤ DataLoader
            self.train_sampler = train_sampler

        def get_train_dataloader(self):
            """
            åªåœ¨è®­ç»ƒé˜¶æ®µåº”ç”¨ WeightedRandomSamplerï¼Œä¿è¯æ¯ä¸ª batch ä¸­å°‘æ•°ç±»æœ‰æ›´é«˜å‡ºç°é¢‘ç‡ã€‚
            éªŒè¯ä¸æµ‹è¯• DataLoader ä»æ²¿ç”¨ Trainer é»˜è®¤å®ç°ï¼ˆé¡ºåº/éšæœºé‡‡æ ·ä½†ä¸é‡é‡‡æ ·ï¼‰ã€‚
            """
            if self.train_sampler is not None:
                return DataLoader(
                    self.train_dataset,
                    batch_size=self.args.train_batch_size,
                    sampler=self.train_sampler,
                    num_workers=self.args.dataloader_num_workers,
                    pin_memory=self.args.dataloader_pin_memory,
                )
            return super().get_train_dataloader()

        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.pop('labels')
            outputs = model(**inputs)
            logits = outputs.logits
            # ç»Ÿä¸€é€šè¿‡ self.loss_fn è®¡ç®—æŸå¤±ï¼ˆæ”¯æŒ FocalLoss / CrossEntropyLossï¼‰
            loss = self.loss_fn(logits, labels)
            return (loss, outputs) if return_outputs else loss
    # è¯„ä¼°æŒ‡æ ‡ï¼ˆä¸ notebooks é£æ ¼ä¸€è‡´ï¼Œç›‘æ§ weighted F1ï¼‰
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=-1)
        # é˜²æ­¢â€œå¡Œé™·åˆ°å•ä¸€ç±»åˆ«â€æ—¶æŒ‡æ ‡è¡¨é¢ç¨³å®šï¼šè¾“å‡ºé¢„æµ‹ç±»åˆ«å¤šæ ·æ€§
        num_pred_classes = int(np.unique(preds).size)
        acc = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)
        return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'pred_classes': num_pred_classes}
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=2,
        learning_rate=2e-5,
        weight_decay=0.01,
        warmup_ratio=0.06,
        lr_scheduler_type='cosine',
        fp16=torch.cuda.is_available(),
        logging_strategy='epoch',
        eval_strategy='epoch',
        save_strategy='epoch',
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model='f1',
        greater_is_better=True,
        report_to='none',
        seed=RANDOM_SEED,
        dataloader_num_workers=0,
        dataloader_pin_memory=True
    )
    logging_cb = DetailedLoggingCallback()
    early_cb = EarlyStoppingCallback(early_stopping_patience=2)
    trainer = CustomLossTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[logging_cb, early_cb],
        loss_fn=loss_fn,
        train_sampler=train_sampler  # ä¼ å…¥ WeightedRandomSampler ä»¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡
    )
    # è®­ç»ƒå‰ sanity check
    print("\nè®­ç»ƒå‰æ£€æŸ¥...")
    sample = train_dataset[0]
    # ç»Ÿè®¡UNKå æ¯”
    with torch.no_grad():
        enc = tokenizer(
            str(train_texts[0]), max_length=512, padding='max_length', truncation=True, return_tensors='pt'
        )
        input_ids = enc['input_ids'][0]
        unk_id = tokenizer.unk_token_id if tokenizer.unk_token_id is not None else -1
        unk_ratio = float((input_ids == unk_id).sum().item()) / float(len(input_ids)) if unk_id >= 0 else 0.0
        print(f"  UNKæ¯”ä¾‹ä¼°è®¡: {unk_ratio:.3f}")
    with torch.no_grad():
        test_inp = {k: v.unsqueeze(0).to(device) for k, v in sample.items() if k != 'labels'}
        test_out = model(**test_inp)
        test_pred = int(torch.argmax(test_out.logits, dim=-1).item())
        print(f"  æ ·æœ¬é¢„æµ‹: çœŸå®={int(sample['labels'].item())}, é¢„æµ‹={test_pred}")
    # è®­ç»ƒ
    trainer.train()
    # ä¿å­˜
    print(f"\nä¿å­˜æ¨¡å‹åˆ° {output_dir}...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    # ä¿å­˜/å¤åˆ¶æ ‡ç­¾æ˜ å°„ï¼Œä¾›æ¨ç†/ç”Ÿæˆå™¨ä½¿ç”¨
    mapping_src = Path(PROCESSED_DATA_BASE_DIR) / dataset_name / 'bert' / mode / 'attack_type_mapping.json'
    mapping_dst = Path(output_dir) / 'attack_type_mapping.json'
    if mode == 'multiclass':
        if mapping_src.exists():
            try:
                shutil.copyfile(mapping_src, mapping_dst)
                print(f"å·²å¤åˆ¶å¤šåˆ†ç±»æ ‡ç­¾æ˜ å°„åˆ°: {mapping_dst}")
            except Exception as e:
                print(f"å¤åˆ¶æ ‡ç­¾æ˜ å°„å¤±è´¥: {e}")
        else:
            print(f"è­¦å‘Š: å¤šåˆ†ç±»æ¨¡å¼æœªæ‰¾åˆ° {mapping_src}ï¼Œä¸‹æ¸¸è‹¥éœ€è¦ç±»åˆ«åæ˜ å°„è¯·é‡æ–°è¿è¡Œé¢„å¤„ç†ç”Ÿæˆã€‚")
    else:
        # äºŒåˆ†ç±»ï¼šæ— è®ºæºæ˜ å°„æ˜¯å¦å­˜åœ¨ï¼Œéƒ½å†™å…¥æ˜ç¡®çš„äºŒåˆ†ç±»æ˜ å°„ï¼Œé¿å…è¯¯ç”¨å¤šåˆ†ç±»æ˜ å°„æ–‡ä»¶
        try:
            with open(mapping_dst, 'w', encoding='utf-8') as f:
                json.dump({"Normal": 0, "Attack": 1}, f, ensure_ascii=False, indent=2)
            print(f"å·²å†™å…¥äºŒåˆ†ç±»æ ‡ç­¾æ˜ å°„: {mapping_dst}")
        except Exception:
            pass
    # ä¿å­˜æ ‡ç­¾æ˜ å°„å ä½ï¼šè‹¥é¢„å¤„ç†é˜¶æ®µå·²ç”Ÿæˆ attack_type_mapping.jsonï¼Œå¯å¤åˆ¶ï¼›
    # æ­¤å¤„ä¿å­˜ num_labels åŠä¼šè¯ç‰¹å¾ä½¿ç”¨æƒ…å†µï¼Œä¾› generator å‚è€ƒ
    training_info = {
        'num_labels': int(num_labels),
        'use_session_tokens': bool(use_session_tokens),
        'session_stat_features': session_stat_cols,
        'loss_type': loss_type
    }
    with open(os.path.join(output_dir, 'training_info.json'), 'w', encoding='utf-8') as f:
        json.dump(training_info, f, ensure_ascii=False, indent=2)
    # è¯¦ç»†è¯„ä¼°
    print("\n" + "="*80)
    print("æœ€ç»ˆè¯„ä¼°")
    print("="*80)
    def evaluate_detailed(dataset, labels, split_name):
        print(f"\n{split_name}é›†:")
        results = trainer.predict(dataset)
        preds = np.argmax(results.predictions, axis=-1)
        # æŒ‡å®š labels ç¡®ä¿å¤šåˆ†ç±»æ—¶æ‰€æœ‰ç±»åˆ«éƒ½å‡ºç°åœ¨æŠ¥å‘Šä¸­
        print(classification_report(labels, preds, labels=list(range(num_labels)), digits=4, zero_division=0))
        return results, preds
    train_results, train_preds = evaluate_detailed(train_dataset, train_labels, "è®­ç»ƒ")
    val_results, val_preds = evaluate_detailed(val_dataset, val_labels, "éªŒè¯")
    test_results, test_preds = evaluate_detailed(test_dataset, test_labels, "æµ‹è¯•")
    # ç»˜å›¾ä¸æ··æ·†çŸ©é˜µ
    plot = logging_cb.get_plot_data()
    if len(plot['epochs']) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes[0, 0].plot(plot['epochs'], plot['train_losses'], 'b-o', label='Train')
        axes[0, 0].plot(plot['epochs'], plot['val_losses'], 'r-o', label='Val')
        axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].set_title('Loss Curves'); axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)
        axes[0, 1].plot(plot['epochs'], plot['val_accs'], 'g-o'); axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('Accuracy'); axes[0, 1].set_title('Validation Accuracy'); axes[0, 1].grid(True, alpha=0.3)
        axes[1, 0].plot(plot['epochs'], plot['val_f1s'], 'm-o'); axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('F1 Score'); axes[1, 0].set_title('Validation F1'); axes[1, 0].grid(True, alpha=0.3)
        cm = confusion_matrix(test_labels, test_preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1], cbar=True)
        axes[1, 1].set_xlabel('Predicted'); axes[1, 1].set_ylabel('True'); axes[1, 1].set_title('Confusion Matrix (Test)')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/training_curves.png", dpi=300, bbox_inches='tight')
        print(f"\nå›¾è¡¨å·²ä¿å­˜: {output_dir}/training_curves.png")
        plt.close()
    # ä¿å­˜æŒ‡æ ‡
    metrics = {
        'train': {k.replace('test_', ''): float(v) for k, v in train_results.metrics.items()},
        'val': {k.replace('test_', ''): float(v) for k, v in val_results.metrics.items()},
        'test': {k.replace('test_', ''): float(v) for k, v in test_results.metrics.items()}
    }
    with open(f"{output_dir}/evaluation_metrics.json", 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    print("\n" + "="*80)
    print("âœ“ è®­ç»ƒä¸è¯„ä¼°å…¨éƒ¨å®Œæˆ")
    print("="*80 + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str, default=MODEL_PATH)
    parser.add_argument('--bert_dir', type=str, default=None,
                       help='æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ® --mode è‡ªåŠ¨é€‰æ‹© processed_data/bert/binary æˆ– multiclassï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ® --mode è‡ªåŠ¨é€‰æ‹© distilbert-finetuned æˆ– distilbert-finetuned-multiclassï¼‰')
    parser.add_argument('--epochs', type=int, default=NUM_EPOCHS)
    parser.add_argument('--mode', type=str, choices=['binary', 'multiclass'], default=DEFAULT_MODE,
                       help='è®­ç»ƒæ¨¡å¼: binary(äºŒåˆ†ç±») æˆ– multiclass(å¤šåˆ†ç±»)ï¼Œè‡ªåŠ¨é€‰æ‹©å¯¹åº”æ•°æ®ç›®å½•')
    parser.add_argument('--dataset', type=str, default=DEFAULT_DATASET,
                       help=f'æ•°æ®é›†åç§°ï¼ˆä¾‹å¦‚: NF-UNSW-NB15-v3ï¼‰ï¼Œé»˜è®¤: {DEFAULT_DATASET}')
    # æ§åˆ¶æ˜¯å¦åœ¨ BERT è¾“å…¥ä¸­æ‹¼æ¥ä¼šè¯å‰ç¼€ç‰¹å¾ä¼ª tokenï¼Œä¾¿äºåšæ¶ˆèå®éªŒ
    parser.add_argument('--use_session_tokens', dest='use_session_tokens', action='store_true')
    parser.add_argument('--no_session_tokens', dest='use_session_tokens', action='store_false')
    parser.set_defaults(use_session_tokens=True)
    # æŸå¤±å‡½æ•°é€‰æ‹©
    parser.add_argument('--loss_type', type=str, choices=['focal', 'focal_weighted', 'ce'],
                       default='focal',
                       help='æŸå¤±å‡½æ•°ç±»å‹: focal(æ— æƒé‡Focal Loss), focal_weighted(æœ‰æƒé‡Focal Loss), ce(çº¯äº¤å‰ç†µ)')
    args = parser.parse_args()

    # æ ¹æ® mode è‡ªåŠ¨é€‰æ‹©è¾“å‡ºç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼‰
    output_dir = args.output_dir if args.output_dir else get_output_dir(args.mode)

    train_distilbert(
        model_path=args.model_path,
        bert_dir=args.bert_dir,  # æ­¤å‚æ•°ç°åœ¨ä»…ç”¨äºå‘åå…¼å®¹ï¼Œå®é™…ç”± dataset å’Œ mode å†³å®š
        output_dir=output_dir,
        num_epochs=args.epochs,
        mode=args.mode,
        dataset_name=args.dataset,
        use_session_tokens=args.use_session_tokens,
        loss_type=args.loss_type
    )